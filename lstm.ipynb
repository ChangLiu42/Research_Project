{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Activation, Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk  #用来分词\n",
    "import collections  #用来统计词频\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len  32\n",
      "nb_words  24490\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "maxlen = 0  #句子最大长度\n",
    "word_freqs = collections.Counter()  #词频\n",
    "num_recs = 0 # 样本数\n",
    "with codecs.open('E:/old collect tweet/train.txt','r+','utf-8') as f:\n",
    "    for line in f:\n",
    "        label, sentence = line.strip().split(\"\\t\")\n",
    "        words = nltk.word_tokenize(sentence.lower())\n",
    "        if len(words) > maxlen:\n",
    "            maxlen = len(words)\n",
    "        for word in words:\n",
    "            word_freqs[word] += 1\n",
    "        num_recs += 1\n",
    "print('max_len ',maxlen)\n",
    "print('nb_words ', len(word_freqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6748\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for a in word_freqs.values():\n",
    "    if a > 2:\n",
    "        i = i + 1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_FEATURES = 6750\n",
    "MAX_SENTENCE_LENGTH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_size = min(MAX_FEATURES, len(word_freqs)) + 2\n",
    "word2index = {x[0]: i+2 for i, x in enumerate(word_freqs.most_common(MAX_FEATURES))}\n",
    "word2index[\"PAD\"] = 0\n",
    "word2index[\"UNK\"] = 1\n",
    "index2word = {v:k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.empty(num_recs,dtype=list)\n",
    "y = np.zeros(num_recs)\n",
    "i=0\n",
    "with open('E:/old collect tweet/train.txt','r+') as f:\n",
    "    for line in f:\n",
    "        label, sentence = line.strip().split(\"\\t\")\n",
    "        words = nltk.word_tokenize(sentence.lower())\n",
    "        seqs = []\n",
    "        for word in words:\n",
    "            if word in word2index:\n",
    "                seqs.append(word2index[word])\n",
    "            else:\n",
    "                seqs.append(word2index[\"UNK\"])\n",
    "        X[i] = seqs\n",
    "        y[i] = int(label)\n",
    "        i += 1\n",
    "X = sequence.pad_sequences(X, maxlen=MAX_SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10668 samples, validate on 2668 samples\n",
      "Epoch 1/10\n",
      "10668/10668 [==============================] - 21s - loss: 0.2110 - acc: 0.9417 - val_loss: 0.1217 - val_acc: 0.9636\n",
      "Epoch 2/10\n",
      "10668/10668 [==============================] - 17s - loss: 0.0808 - acc: 0.9753 - val_loss: 0.1130 - val_acc: 0.9644\n",
      "Epoch 3/10\n",
      "10668/10668 [==============================] - 18s - loss: 0.0461 - acc: 0.9859 - val_loss: 0.1101 - val_acc: 0.9719\n",
      "Epoch 4/10\n",
      "10668/10668 [==============================] - 18s - loss: 0.0275 - acc: 0.9921 - val_loss: 0.1697 - val_acc: 0.9618\n",
      "Epoch 5/10\n",
      "10668/10668 [==============================] - 18s - loss: 0.0203 - acc: 0.9941 - val_loss: 0.1596 - val_acc: 0.9610\n",
      "Epoch 6/10\n",
      "10668/10668 [==============================] - 18s - loss: 0.0149 - acc: 0.9953 - val_loss: 0.1912 - val_acc: 0.9659\n",
      "Epoch 7/10\n",
      "10668/10668 [==============================] - 17s - loss: 0.0113 - acc: 0.9967 - val_loss: 0.1952 - val_acc: 0.9588\n",
      "Epoch 8/10\n",
      "10668/10668 [==============================] - 17s - loss: 0.0104 - acc: 0.9969 - val_loss: 0.2179 - val_acc: 0.9580\n",
      "Epoch 9/10\n",
      "10668/10668 [==============================] - 18s - loss: 0.0076 - acc: 0.9978 - val_loss: 0.2345 - val_acc: 0.9569\n",
      "Epoch 10/10\n",
      "10668/10668 [==============================] - 17s - loss: 0.0055 - acc: 0.9982 - val_loss: 0.2269 - val_acc: 0.9573\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x208f145d7b8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EMBEDDING_SIZE = 128\n",
    "HIDDEN_LAYER_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, EMBEDDING_SIZE,input_length=MAX_SENTENCE_LENGTH))\n",
    "model.add(LSTM(HIDDEN_LAYER_SIZE, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(Xtrain, ytrain, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,validation_data=(Xtest, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2624/2668 [============================>.] - ETA: 0s\n",
      "Test score: 0.227, accuracy: 0.957\n",
      "predict label   label      text\n",
      "      0           0     rt stop trying to look perfect the most flawless bananas usually taste gross\n",
      "      0           0     rt my heart is heavy rip chester\n",
      "      0           0     an UNK bro version of UNK love this glass it 's so heavy feel like jr UNK at the oil\n",
      "      0           0     life 's like head full of amazing thick hair people trying to count your whole head as or UNK like really split some UNK\n",
      "      0           0     # nigeria # rain # UNK # heavyrain # kid # dancing love the rain love the dancing ps rains\n",
      "      1           1     did someone say breakfast pick up bag of our mouth-watering thick cut UNK bacon express\n",
      "      0           0     helped nearby drivers by reporting heavy traffic jam on manor pl edinburgh on drive social\n",
      "      0           0     acr 125 engross UNK need awareness week\n",
      "      0           0     rt with the help of to reduce food waste 50 by UNK read their food waste report\n",
      "      0           0     today 's focus was snatch work followed by some heavy deadlift for we finished with fun\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(Xtest, ytest, batch_size=BATCH_SIZE)\n",
    "print(\"\\nTest score: %.3f, accuracy: %.3f\" % (score, acc))\n",
    "print('{}   {}      {}'.format('predict label','label','text'))\n",
    "for i in range(10):\n",
    "    idx = np.random.randint(len(Xtest))\n",
    "    xtest = Xtest[idx].reshape(1,30)\n",
    "    ylabel = ytest[idx]\n",
    "    ypred = model.predict(xtest)[0][0]\n",
    "    sent = \" \".join([index2word[x] for x in xtest[0] if x != 0])\n",
    "    print('      {}           {}     {}'.format(int(round(ypred)), int(ylabel), sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "10030\n",
      "20030\n",
      "30030\n",
      "40030\n",
      "50030\n",
      "60030\n",
      "70030\n",
      "80030\n",
      "90030\n",
      "100030\n",
      "110030\n",
      "120030\n",
      "130030\n",
      "140030\n",
      "150030\n",
      "153984\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# oldtweet = open('E:/middleTwitter.json','r',encoding=\"utf8\")\n",
    "# i = 0\n",
    "# alltext = []\n",
    "# for line in oldtweet:\n",
    "#     try:\n",
    "#         new = json.loads(line,encoding = 'utf8')\n",
    "#         alltext.append(new['text'])\n",
    "#         i = i + 1\n",
    "#         if i % 10000 == 30:\n",
    "#             print(i)\n",
    "#     except BaseException as e:\n",
    "#         print('Error on_data:'+str(e)) \n",
    "# oldtweet.close()\n",
    "# print(len(alltext))\n",
    "import json\n",
    "oldtweet = open('E:/old collect tweet/hysimpcollected.json','r',encoding=\"utf8\")\n",
    "i = 0\n",
    "alltext = []\n",
    "for line in oldtweet:\n",
    "    try:\n",
    "        new = json.loads(line,encoding = 'utf8')\n",
    "        alltext.append(new['text'])\n",
    "        i = i + 1\n",
    "        if i % 10000 == 30:\n",
    "            print(i)\n",
    "    except BaseException as e:\n",
    "        print('Error on_data:'+str(e)) \n",
    "oldtweet.close()\n",
    "print(len(alltext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stopwords = set(stopwords.words('english'))\n",
    "delpunc = string.punctuation\n",
    "    \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    " \n",
    "# print(preprocess(tweet))\n",
    "def rmhttp(list_str):\n",
    "    rmhttp = []\n",
    "    for a in list_str:\n",
    "        if 'http://' in a:\n",
    "            continue\n",
    "        elif 'https://' in a:\n",
    "            continue\n",
    "        elif '@' in a:\n",
    "            continue\n",
    "        elif len(a) == 1:\n",
    "            if ord(a) < 32 or ord(a) > 126:\n",
    "                continue\n",
    "#         elif a.lower() in stopwords:\n",
    "#             continue\n",
    "#         elif a in delpunc:\n",
    "#             continue        \n",
    "        else:\n",
    "            rmhttp.append(a)\n",
    "    return ' '.join(rmhttp)\n",
    "tweettest = u'RT @marcobonzanini: just an example! :D http://example.com #NLP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153984\n"
     ]
    }
   ],
   "source": [
    "cleantext = []\n",
    "for a in alltext:\n",
    "    cleantext.append(rmhttp(preprocess(a)))\n",
    "print(len(cleantext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not   Couldn't have got through that slab without this bad boy from our mates haleyhire I'm\n",
      "not   It's the th of April had seriously bad dream last night killed spider the night before in my dream spiders\n",
      "not   Oh My Yoga have my very own yoga page on Instagram FOLLOW omy_australia if you'd like\n",
      "not   am coffee fanatic Once you go to proper coffee you can't go back You cannot go back\n",
      "not   Bought jacket City Chic in Docklands VIC\n",
      "not   Written in style John Clarke would appreciate\n",
      "not   bigbosbbq No chance was sacrificing this hot meal for some fancy photos tonight Double\n",
      "not   Working in an industry where got to meet lot of tourist from\n",
      "not   #best Apr 10 #melbourne #australia #travel #money offers app guides you to #cheap #forex\n",
      "not   #Repost EHPlabs ambassador and my fellow My favorite post workout supplement\n",
      "not   Beer was made by men wine by God Yarra Valley Wineries Tour\n"
     ]
    }
   ],
   "source": [
    "# INPUT_SENTENCES = ['I love reading.','You are so boring.']\n",
    "INPUT_SENTENCES = cleantext\n",
    "XX = np.empty(len(INPUT_SENTENCES),dtype=list)\n",
    "i=0\n",
    "for sentence in  INPUT_SENTENCES:\n",
    "    words = nltk.word_tokenize(sentence.lower())\n",
    "    seq = []\n",
    "    for word in words:\n",
    "        if word in word2index:\n",
    "            seq.append(word2index[word])\n",
    "        else:\n",
    "            seq.append(word2index['UNK'])\n",
    "    XX[i] = seq\n",
    "    i+=1\n",
    "\n",
    "XX = sequence.pad_sequences(XX, maxlen=MAX_SENTENCE_LENGTH)\n",
    "labels = [int(round(x[0])) for x in model.predict(XX) ]\n",
    "label2word = {1:'fat fastfood', 0:'not'}\n",
    "a = 0\n",
    "for i in range(len(INPUT_SENTENCES)):\n",
    "    print('{}   {}'.format(label2word[labels[i]], INPUT_SENTENCES[i]))\n",
    "    a = a + 1\n",
    "    if a > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18775\n",
      "153984\n",
      "not\n"
     ]
    }
   ],
   "source": [
    "fastnum = 0\n",
    "allnum = 0\n",
    "for i in range(len(INPUT_SENTENCES)):\n",
    "    allnum = allnum + 1\n",
    "    if len(label2word[labels[i]]) == 12:\n",
    "        fastnum = fastnum + 1\n",
    "print(fastnum)\n",
    "print(allnum)\n",
    "print(label2word[labels[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "10030\n",
      "20030\n",
      "30030\n",
      "40030\n",
      "50030\n",
      "60030\n",
      "70030\n",
      "80030\n",
      "90030\n",
      "100030\n",
      "110030\n",
      "120030\n",
      "130030\n",
      "140030\n",
      "150030\n",
      "complete\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import codecs\n",
    "firstfile = open('E:/old collect tweet/hysimpcollected.json','r',encoding=\"utf8\")\n",
    "secondfile = codecs.open('E:/old collect tweet/hylabelledTwitter.json','w',encoding='utf-8')\n",
    "alltext = []\n",
    "i = 0\n",
    "for line in firstfile:\n",
    "    text = []\n",
    "    try:\n",
    "        tweet = json.loads(line)\n",
    "        obj = {\"created_at\":tweet['created_at'],\"label\":label2word[labels[i]],\"text\":tweet['text'],\"id\":tweet['id'],\"coordinate\":tweet['coordinates']}\n",
    "#             print obj\n",
    "        aaaa = json.dumps(obj, ensure_ascii=False)\n",
    "        secondfile.write(aaaa+'\\n')\n",
    "        i = i+1\n",
    "        if i % 10000 == 30:\n",
    "            print(i)\n",
    "    except BaseException as e:\n",
    "        print('Error on_data:'+str(e))\n",
    "print('complete')\n",
    "firstfile.close()\n",
    "secondfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
